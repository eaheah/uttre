{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras - adapted from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly'\n",
    "    def __init__(self, list_IDs, batch_size=32, dim=(95,95), n_channels=3, \n",
    "                 datapath='/vagrant/imgs/training_data/training_data/aligned',\n",
    "                 attribute_path='/vagrant/imgs/list_attr_celeba.csv',\n",
    "                 label_size=40, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.label_size = label_size\n",
    "        self.datapath = datapath\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "        self.df = pd.read_csv(attribute_path)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def get_numpy_image(self, image_path):\n",
    "        img =  cv2.imread(os.path.join(self.datapath, image_path))\n",
    "        return img / 255\n",
    "    \n",
    "    def get_label(self, image_path):\n",
    "        if 'png' in image_path:\n",
    "            image_path = image_path.replace('png', 'jpg')\n",
    "        row = self.df.loc[self.df['image_id'] == image_path]\n",
    "        label = np.array(row.values.tolist()[0][1:])\n",
    "#         print(\"label: {}\".format(d))\n",
    "        label[label < 0] = 0\n",
    "        return label\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        Y = np.empty((self.batch_size, self.label_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            X[i,] = self.get_numpy_image(ID)\n",
    "            Y[i,] = self.get_label(ID)\n",
    "            \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionDataGenerator(DataGenerator):\n",
    "    def __init__(self, list_IDs, dim=(95,95), n_channels=3, \n",
    "                 datapath='/vagrant/imgs/training_data/training_data/aligned'):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = len(list_IDs)\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = False\n",
    "        self.datapath = datapath\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            X[i,] = self.get_numpy_image(ID)\n",
    "            \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_partition(amount='all', datapath='/vagrant/imgs/training_data/training_data/aligned', split=(60, 20, 20)):\n",
    "    directory = os.listdir(datapath)\n",
    "    shuffle(directory)\n",
    "    if amount != 'all':\n",
    "        directory = directory[:amount]\n",
    "    l = len(directory)\n",
    "    train = int(l *split[0]/100)\n",
    "    val = int(l * split[1]/100) + train\n",
    "    test = int(l * split[2]/100) + val\n",
    "    \n",
    "    return {\n",
    "        \"train\": directory[:train],\n",
    "        \"validation\": directory[train:val],\n",
    "        \"test\": directory[val:]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_generators, checkpoint_path, patience=20, period=5, workers=8, epochs=100, verbose=1):\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "\n",
    "    cp_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_path, verbose=verbose, save_weights_only=True,\n",
    "        period=period)\n",
    "    \n",
    "    history = model.fit_generator(generator=data_generators['training_generator'],\n",
    "                        validation_data=data_generators['validation_generator'],\n",
    "                        use_multiprocessing=True,\n",
    "                        workers=workers,\n",
    "                        epochs=epochs,\n",
    "                        verbose=verbose,\n",
    "                        callbacks=[early_stop, cp_callback])\n",
    "\n",
    "    result = model.evaluate_generator(generator=data_generators['test_generator'], verbose=verbose)\n",
    "    predictions = model.predict_generator(generator=data_generators['predition_generator'], verbose=verbose)\n",
    "    return history, result, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def md(directory):\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "224/224 [==============================] - 3216s 14s/step - loss: 0.5042 - acc: 0.8266 - val_loss: 0.3644 - val_acc: 0.8479\n",
      "Epoch 2/100\n",
      "224/224 [==============================] - 2988s 13s/step - loss: 0.3529 - acc: 0.8497 - val_loss: 0.3392 - val_acc: 0.8549\n",
      "Epoch 3/100\n",
      "224/224 [==============================] - 2981s 13s/step - loss: 0.3365 - acc: 0.8553 - val_loss: 0.3468 - val_acc: 0.8497\n",
      "Epoch 4/100\n",
      "224/224 [==============================] - 2994s 13s/step - loss: 0.3292 - acc: 0.8579 - val_loss: 0.3235 - val_acc: 0.8600\n",
      "Epoch 5/100\n",
      "223/224 [============================>.] - ETA: 10s - loss: 0.3247 - acc: 0.8597\n",
      "Epoch 00005: saving model to checkpoints/model1/cp-0005.ckpt\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f5e71899ef0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "224/224 [==============================] - 3088s 14s/step - loss: 0.3247 - acc: 0.8597 - val_loss: 0.3254 - val_acc: 0.8587\n",
      "Epoch 6/100\n",
      "224/224 [==============================] - 3268s 15s/step - loss: 0.3201 - acc: 0.8615 - val_loss: 0.3181 - val_acc: 0.8619\n",
      "Epoch 7/100\n",
      "139/224 [=================>............] - ETA: 15:16 - loss: 0.3193 - acc: 0.8617"
     ]
    }
   ],
   "source": [
    "# 'adapted from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly'\n",
    "import numpy as np\n",
    "from models import model1, model2\n",
    "# Parameters\n",
    "params = {'dim': (95,95),\n",
    "          'batch_size': 512,\n",
    "          'n_channels': 3,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Datasets\n",
    "partition = create_partition()\n",
    "\n",
    "# Generators\n",
    "data_generators = {\n",
    "    'training_generator': DataGenerator(partition['train'], **params),\n",
    "    'validation_generator': DataGenerator(partition['validation'], **params),\n",
    "    'test_generator': DataGenerator(partition['test'], **params),\n",
    "    'predition_generator': PredictionDataGenerator(partition['test'])\n",
    "}\n",
    "\n",
    "model = model1.create_model()\n",
    "md('checkpoints/model1')\n",
    "history, result, predictions = evaluate_model(model, data_generators, 'checkpoints/model1/cp-{epoch:04d}.ckpt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md('saved_models')\n",
    "model.save('saved_models/model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'checkpoints/model1/cp-0045.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path)\n",
    "result = model.evaluate_generator(generator=data_generators['test_generator'], verbose=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = tf.train.latest_checkpoint('checkpoints/model1')\n",
    "model = model1.create_model()\n",
    "model.load_weights(latest)\n",
    "result = model.evaluate_generator(generator=data_generators['test_generator'], verbose=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('saved_models/model1.h5')\n",
    "result = model.evaluate_generator(generator=data_generators['test_generator'], verbose=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
