{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras - adapted from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly'\n",
    "    def __init__(self, list_IDs, batch_size=32, dim=(95,95), n_channels=3, \n",
    "                 datapath='/vagrant/imgs/training_data/training_data/aligned',\n",
    "                 attribute_path='/vagrant/imgs/list_attr_celeba.csv',\n",
    "                 label_size=40, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.label_size = label_size\n",
    "        self.datapath = datapath\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "        self.df = pd.read_csv(attribute_path)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def get_numpy_image(self, image_path):\n",
    "        img =  cv2.imread(os.path.join(self.datapath, image_path))\n",
    "        return img / 255\n",
    "    \n",
    "    def get_label(self, image_path):\n",
    "        if 'png' in image_path:\n",
    "            image_path = image_path.replace('png', 'jpg')\n",
    "        row = self.df.loc[self.df['image_id'] == image_path]\n",
    "        label = np.array(row.values.tolist()[0][1:])\n",
    "#         print(\"label: {}\".format(d))\n",
    "        label[label < 0] = 0\n",
    "        return label\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        Y = np.empty((self.batch_size, self.label_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            X[i,] = self.get_numpy_image(ID)\n",
    "            Y[i,] = self.get_label(ID)\n",
    "            \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionDataGenerator(DataGenerator):\n",
    "    def __init__(self, list_IDs, dim=(95,95), n_channels=3, \n",
    "                 datapath='/vagrant/imgs/training_data/training_data/aligned'):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = len(list_IDs)\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = False\n",
    "        self.datapath = datapath\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            X[i,] = self.get_numpy_image(ID)\n",
    "            \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_partition(amount='all', datapath='/vagrant/imgs/training_data/training_data/aligned', split=(60, 20, 20)):\n",
    "    directory = os.listdir(datapath)\n",
    "    shuffle(directory)\n",
    "    if amount != 'all':\n",
    "        directory = directory[:amount]\n",
    "    l = len(directory)\n",
    "    train = int(l *split[0]/100)\n",
    "    val = int(l * split[1]/100) + train\n",
    "    test = int(l * split[2]/100) + val\n",
    "    \n",
    "    return {\n",
    "        \"train\": directory[:train],\n",
    "        \"validation\": directory[train:val],\n",
    "        \"test\": directory[val:]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape=(95,95,3), optimizer=tf.train.AdamOptimizer, loss='binary_crossentropy', metrics=['accuracy']):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=input_shape),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(40, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer(), \n",
    "                  loss=loss,\n",
    "                  metrics=metrics)    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_generators, patience=20, workers=8, epochs=100, verbose=1):\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "    history = model.fit_generator(generator=data_generators['training_generator'],\n",
    "                        validation_data=data_generators['validation_generator'],\n",
    "                        use_multiprocessing=True,\n",
    "                        workers=workers,\n",
    "                        epochs=epochs,\n",
    "                        verbose=verbose,\n",
    "                        callbacks=[early_stop])\n",
    "\n",
    "    result = model.evaluate_generator(generator=data_generators['test_generator'], verbose=verbose)\n",
    "    predictions = model.predict_generator(generator=data_generators['predition_generator'], verbose=verbose)\n",
    "    return history, result, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair']\n",
      "['Straight_Hair', 'Wavy_Hair']\n"
     ]
    }
   ],
   "source": [
    "def determine_attributes(prediction):\n",
    "    label_names = ['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', \n",
    "                   'Big_Nose', 'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin', \n",
    "                   'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open', \n",
    "                   'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', \n",
    "                   'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', \n",
    "                   'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young']\n",
    "    label_dict = {i:name for i,name in enumerate(label_names)}\n",
    "    reverse_label_dict = {name:i for i, name in label_dict.items()}\n",
    "    \n",
    "    related = [\n",
    "        ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair'],\n",
    "        ['Straight_Hair', 'Wavy_Hair']\n",
    "    ]\n",
    "\n",
    "    reinforcement = {\n",
    "        '5_o_Clock_Shadow': .1,\n",
    "        'Goatee': .1,\n",
    "        'Mustache': .1,\n",
    "        'Sideburns': .1,\n",
    "        'Wearing_Necktie': .1,\n",
    "        'Heavy_Makeup': -.1,\n",
    "        'Wearing_Earrings': -.1,\n",
    "        'Wearing_Lipstick': -.1,\n",
    "        'Wearing_Necklace': -.1,\n",
    "    }\n",
    "    \n",
    "    threshold = .3\n",
    "    male = 20\n",
    "    \n",
    "    predition = prediction.tolist()\n",
    "    intermediary = []\n",
    "    \n",
    "    # high threshold\n",
    "    for value in prediction:\n",
    "        if value < threshold:\n",
    "            intermediary.append(0)\n",
    "        elif value > 1 - threshold:\n",
    "            intermediary.append(1)\n",
    "        else:\n",
    "            intermediary.append(value)\n",
    "\n",
    "    # reinforce gender  \n",
    "    if intermediary[male] not in (1,0):\n",
    "        for key in reinforcement:\n",
    "            if intermediary[reverse_label_dict[key]] == 1:\n",
    "                intermediary[male] += reinforcement[key]\n",
    "    if intermediary[male] < threshold:\n",
    "        intermediary[male] = 0\n",
    "    elif intermediary[male] > 1 - threshold:\n",
    "        intermediary[male] = 1\n",
    "        \n",
    "    # remove related if one is strong\n",
    "    for d in related:\n",
    "        print(d)\n",
    "        if any([intermediary[reverse_label_dict[name]] == 1 for name in d]):\n",
    "            for name in d:\n",
    "                if name != 1:\n",
    "                    del intermediary[reverse_label_dict[i]]\n",
    "        \n",
    "    results = {\n",
    "        'sure': {'pos': [], 'neg': []},\n",
    "        'unsure': {'pos': [], 'neg': []}\n",
    "    }\n",
    "    \n",
    "    for i, value in enumerate(intermediary):\n",
    "        name = label_dict[i]\n",
    "        if value == 0:\n",
    "            results['sure']['neg'].append(name)\n",
    "        elif value == 1:\n",
    "            results['sure']['pos'].append(name)\n",
    "        elif value < .5:\n",
    "            results['unsure']['neg'].append(name)\n",
    "        else:\n",
    "            results['unsure']['pos'].append(name)\n",
    "    \n",
    "    return results\n",
    "\n",
    "    \n",
    "readable_attrs = determine_attributes(preditions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 7/12 [================>.............] - ETA: 1s - loss: 1.9574 - acc: 0.6914 Epoch 1/100\n",
      "12/12 [==============================] - 5s 379ms/step - loss: 1.8526 - acc: 0.7162 - val_loss: 1.5110 - val_acc: 0.7700\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 1.1105 - acc: 0.7683 - val_loss: 0.8622 - val_acc: 0.7412\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 0.6750 - acc: 0.7858 - val_loss: 0.5255 - val_acc: 0.7450\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 0.5319 - acc: 0.7421 - val_loss: 0.5524 - val_acc: 0.7688\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 0.4845 - acc: 0.7796 - val_loss: 0.4743 - val_acc: 0.7750\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 0.4379 - acc: 0.8171 - val_loss: 0.4745 - val_acc: 0.7850\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 2s 170ms/step - loss: 0.4169 - acc: 0.8154 - val_loss: 0.4434 - val_acc: 0.7925\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 3s 214ms/step - loss: 0.4039 - acc: 0.8246 - val_loss: 0.4559 - val_acc: 0.7875\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 2s 159ms/step - loss: 0.3930 - acc: 0.8267 - val_loss: 0.4604 - val_acc: 0.7975\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 0.3813 - acc: 0.8358 - val_loss: 0.4560 - val_acc: 0.8038\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 2s 163ms/step - loss: 0.3723 - acc: 0.8350 - val_loss: 0.4690 - val_acc: 0.8013\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 0.3585 - acc: 0.8458 - val_loss: 0.4522 - val_acc: 0.7963\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 3s 238ms/step - loss: 0.3504 - acc: 0.8483 - val_loss: 0.4647 - val_acc: 0.7912\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 0.3490 - acc: 0.8508 - val_loss: 0.4444 - val_acc: 0.8075\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 2s 180ms/step - loss: 0.3411 - acc: 0.8538 - val_loss: 0.4580 - val_acc: 0.8000\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 0.3336 - acc: 0.8608 - val_loss: 0.4524 - val_acc: 0.7912\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 0.3364 - acc: 0.8550 - val_loss: 0.4578 - val_acc: 0.7900\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 0.3267 - acc: 0.8621 - val_loss: 0.4442 - val_acc: 0.7900\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 0.3288 - acc: 0.8579 - val_loss: 0.4426 - val_acc: 0.7937\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 0.3214 - acc: 0.8592 - val_loss: 0.4678 - val_acc: 0.7875\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 2s 182ms/step - loss: 0.3207 - acc: 0.8637 - val_loss: 0.4568 - val_acc: 0.8087\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 3s 213ms/step - loss: 0.3062 - acc: 0.8783 - val_loss: 0.4432 - val_acc: 0.8038\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 2s 202ms/step - loss: 0.3058 - acc: 0.8717 - val_loss: 0.4521 - val_acc: 0.7912\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 2s 160ms/step - loss: 0.3071 - acc: 0.8754 - val_loss: 0.4757 - val_acc: 0.7975\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 3s 213ms/step - loss: 0.3031 - acc: 0.8704 - val_loss: 0.4631 - val_acc: 0.7975\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 0.2960 - acc: 0.8792 - val_loss: 0.4717 - val_acc: 0.8025\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 0.2938 - acc: 0.8762 - val_loss: 0.4437 - val_acc: 0.8038\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 0.2931 - acc: 0.8767 - val_loss: 0.4742 - val_acc: 0.7925\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 0.2910 - acc: 0.8750 - val_loss: 0.4712 - val_acc: 0.7837\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 0.2944 - acc: 0.8717 - val_loss: 0.4542 - val_acc: 0.7925\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 0.2890 - acc: 0.8775 - val_loss: 0.4639 - val_acc: 0.7987\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 0.2794 - acc: 0.8817 - val_loss: 0.4682 - val_acc: 0.7975\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 0.2906 - acc: 0.8787 - val_loss: 0.4967 - val_acc: 0.8025\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 2s 191ms/step - loss: 0.2902 - acc: 0.8767 - val_loss: 0.4970 - val_acc: 0.7962\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 2s 204ms/step - loss: 0.2852 - acc: 0.8767 - val_loss: 0.4622 - val_acc: 0.8000\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 2s 164ms/step - loss: 0.2722 - acc: 0.8850 - val_loss: 0.5081 - val_acc: 0.7838\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 0.2652 - acc: 0.8867 - val_loss: 0.4685 - val_acc: 0.7938\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 0.2592 - acc: 0.8929 - val_loss: 0.4661 - val_acc: 0.7963\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 3s 212ms/step - loss: 0.2492 - acc: 0.9000 - val_loss: 0.4594 - val_acc: 0.7987\n",
      "4/4 [==============================] - 1s 343ms/step\n",
      "(20, 95, 95, 3)\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "(20, 95, 95, 3)\n"
     ]
    }
   ],
   "source": [
    "# 'adapted from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly'\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Parameters\n",
    "params = {'dim': (95,95),\n",
    "          'batch_size': 5,\n",
    "          'n_channels': 3,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Datasets\n",
    "partition = create_partition(amount=100)\n",
    "\n",
    "# Generators\n",
    "data_generators = {\n",
    "    'training_generator': DataGenerator(partition['train'], **params),\n",
    "    'validation_generator': DataGenerator(partition['validation'], **params),\n",
    "    'test_generator': DataGenerator(partition['test'], **params),\n",
    "    'predition_generator': PredictionDataGenerator(partition['test'])\n",
    "}\n",
    "\n",
    "model = create_model()\n",
    "history, result, predictions = evaluate_model(model, data_generators)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_8 (Flatten)          (None, 27075)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               3465728   \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 40)                5160      \n",
      "=================================================================\n",
      "Total params: 3,470,888\n",
      "Trainable params: 3,470,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4558965563774109, 0.7899999916553497]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12588444, 0.32350117, 0.6725941 , 0.19547054, 0.01614203,\n",
       "       0.02820873, 0.1741563 , 0.26098672, 0.252925  , 0.18323612,\n",
       "       0.00596957, 0.3818913 , 0.07264109, 0.03637172, 0.08249784,\n",
       "       0.03903544, 0.0471328 , 0.02642897, 0.23665993, 0.7427596 ,\n",
       "       0.70710564, 0.3255983 , 0.0445119 , 0.07717391, 0.72756755,\n",
       "       0.38242835, 0.02687896, 0.11712329, 0.05296161, 0.05350516,\n",
       "       0.03041244, 0.71803063, 0.14684525, 0.17305525, 0.2152747 ,\n",
       "       0.09664284, 0.26611808, 0.04464798, 0.14673384, 0.7768357 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preditions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
